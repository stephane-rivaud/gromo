{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from gromo.tools import *\n",
    "import tensorly as tl\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_close(a, b, msg=\"\", atol=1e-8, rtol=1e-5):\n",
    "    assert a.shape == b.shape, f\"{msg} (shape mismatch : {a.shape} != {b.shape})\"\n",
    "    assert torch.allclose(a, b, atol=atol, rtol=rtol), f\"{msg} (||.||_inf = {torch.max(torch.abs(a - b)):2e}, ||.||_0 = {torch.sum(torch.abs(a - b)) / a.numel():2e} %, ||.||_2 = {torch.norm(a - b) / torch.norm(b):2e})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemma\n",
    "\n",
    "Conv o AvgPool = AvgPool o Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "cin = 4\n",
    "cout = 5\n",
    "ĥd = 3\n",
    "wd = 2\n",
    "h = 22\n",
    "w = 31\n",
    "px = 5\n",
    "py = 6\n",
    "n = 10\n",
    "\n",
    "# Create a random image\n",
    "x = torch.randn(n, cin, h, w)\n",
    "\n",
    "# Create the layers\n",
    "conv = nn.Conv2d(cin, cout, wd)\n",
    "pool = nn.AvgPool2d((px, py), stride=(1, 1))\n",
    "\n",
    "# Forward pass\n",
    "y1 = conv(pool(x))\n",
    "y2 = pool(conv(x))\n",
    "\n",
    "# Check the shapes\n",
    "assert y1.shape == y2.shape, f\"Shapes are different: {y1.shape=} != {y2.shape=}\"\n",
    "\n",
    "assert_close(y1, y2, \"There is no commutativity\", atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv to conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start with x.shape=torch.Size([2, 5, 9, 10])\n",
      "Apply conv1=Conv2d(5, 6, kernel_size=(3, 5), stride=(1, 1), bias=False) (output shape: torch.Size([2, 6, 7, 6]))\n",
      "Apply conv2=Conv2d(6, 7, kernel_size=(5, 3), stride=(1, 1), bias=False) (output shape: torch.Size([2, 7, 3, 4]))\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "c0 = 5\n",
    "c1 = 6\n",
    "c2 = 7\n",
    "\n",
    "hd1 = 3\n",
    "wd1 = 5\n",
    "hd2 = 5\n",
    "wd2 = 3\n",
    "\n",
    "h0 = 9\n",
    "w0 = 10\n",
    "\n",
    "# h0 = 21\n",
    "# w0 = 30\n",
    "\n",
    "n = 2\n",
    "\n",
    "# Create a random image\n",
    "\n",
    "x = torch.randn(n, c0, h0, w0)\n",
    "pre_conv = nn.Conv2d(c0, c0, (3, 3), padding=1, bias=False)\n",
    "x = pre_conv(x) # add unisotropicity\n",
    "\n",
    "# Create the layers\n",
    "conv1 = nn.Conv2d(c0, c1, (hd1, wd1), bias=False)\n",
    "assert conv1.weight.shape == (c1, c0, hd1, wd1), f\"Wrong shape: {conv1.weight.shape=}\"\n",
    "assert torch.allclose(torch.nn.functional.conv2d(input=x, \n",
    "                                                 weight=conv1.weight, \n",
    "                                                 bias=conv1.bias,\n",
    "                                                 stride=conv1.stride, \n",
    "                                                 padding=conv1.padding, \n",
    "                                                 dilation=conv1.dilation), \n",
    "                                                 conv1(x)), \"Error in the convolution\"\n",
    "\n",
    "conv2 = nn.Conv2d(c1, c2, (hd2, wd2), bias=False)\n",
    "\n",
    "# Forward pass\n",
    "print(f\"Start with {x.shape=}\")\n",
    "print(f\"Apply {conv1=} (output shape: {conv1(x).shape})\")\n",
    "print(f\"Apply {conv2=} (output shape: {conv2(conv1(x)).shape})\")\n",
    "\n",
    "h1, w1 = conv1(x).shape[-2:]\n",
    "h2, w2 = conv2(conv1(x)).shape[-2:]\n",
    "\n",
    "\n",
    "y_th = conv2(conv1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unfolded = torch.nn.functional.unfold(x, (hd1, wd1), padding=conv1.padding, stride=conv1.stride, dilation=conv1.dilation)\n",
    "assert x_unfolded.shape == (n, c0 * hd1 * wd1, h1 * w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{B} \\in (n, C[-1] dd, HW)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unfolded_flat = x_unfolded.permute(0, 2, 1).flatten(end_dim=1)\n",
    "assert x_unfolded_flat.shape == (n * h1 * w1, c0 * hd1 * wd1), f\"{x_unfolded_flat.shape=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1_flat = conv1.weight.flatten(start_dim=1)\n",
    "assert theta1_flat.shape == (c1, c0 * hd1 * wd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_th_half = conv1(x)\n",
    "assert y_th_half.shape == (n, c1, h1, w1)\n",
    "\n",
    "y_th_half_flat = y_th_half.flatten(start_dim=2)\n",
    "assert y_th_half_flat.shape == (n, c1, h1 * w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_f_half1 = torch.einsum(\"iam, ca -> icm\", x_unfolded, theta1_flat)\n",
    "assert y_f_half1.shape == (n, c1, h1 * w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_close(y_f_half1, y_th_half_flat, \"Error in the first convolution\", atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_f_half_tl = tl.tenalg.mode_dot(x_unfolded, theta1_flat, mode=1)\n",
    "assert y_f_half_tl.shape == (n, c1, h1 * w1)\n",
    "assert_close(y_f_half_tl, y_th_half_flat, \"Error in the mode_dot\", atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_f_half2 = x_unfolded_flat @ theta1_flat.T\n",
    "assert y_f_half2.shape == (n * h1 * w1, c1)\n",
    "y_half2 = y_f_half2.reshape(n, h1 * w1, c1).permute(0, 2, 1)\n",
    "assert_close(y_half2, y_th_half_flat, \"Error in the matrix multiplication\", atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Conv_{\\Theta_1}(B) \\sim \\hat{B}^{iam} \\Theta_1^{ca} = \\hat{B} \\times_1 \\Theta_1 \\sim \\hat{B}_F \\times \\Theta_1^T$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = x_unfolded_flat.T @ x_unfolded_flat\n",
    "assert s.shape == (c0 * hd1 * wd1, c0 * hd1 * wd1)\n",
    "\n",
    "\n",
    "y_th_half_flat_2 = y_th_half_flat.permute(0, 2, 1).flatten(end_dim=1)\n",
    "assert y_th_half_flat_2.shape == (n * h1 * w1, c1)\n",
    "\n",
    "m = y_th_half_flat_2.T @ x_unfolded_flat\n",
    "assert m.shape == (c1, c0 * hd1 * wd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = torch.einsum(\"iam, ibm -> ab\", x_unfolded, x_unfolded)\n",
    "assert s1.shape == (c0 * hd1 * wd1, c0 * hd1 * wd1)\n",
    "\n",
    "n1 = torch.einsum(\"iam, icm -> ca\", x_unfolded, y_th_half_flat)\n",
    "assert n1.shape == (c1, c0 * hd1 * wd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_close(n1, m, \"Error in the einsum\", atol=1e-6)\n",
    "assert_close(s1, s, \"Error in the einsum\", atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4402e-05, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_star = torch.linalg.solve(s, m.T)\n",
    "assert w_star.shape == (c0 * hd1 * wd1, c1)\n",
    "w_star_r = w_star.reshape(c0, hd1, wd1, c1).permute(3, 0, 1, 2)\n",
    "\n",
    "conv_star = nn.Conv2d(c0, c1, (hd1, wd1), bias=False)\n",
    "conv_star.weight.data = w_star_r\n",
    "torch.norm(conv_star(x) - conv1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double convolution as a linear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show that $(Conv_1 \\circ Conv_2)(X) = X \\times \\Theta$ with the correct reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = K1 K2 = A B\n",
    "theta = torch.einsum(\"cdkl, ecxy -> dklxye\", conv1.weight, conv2.weight)\n",
    "assert theta.shape == (c0, hd1, wd1, hd2, wd2, c2)\n",
    "\n",
    "theta = theta.flatten(3, 4).flatten(0, 2)\n",
    "assert theta.shape == (c0 * hd1 * wd1, hd2 * wd2, c2), f\"{theta.shape=}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Theta^{dklxye} = W^{cdkl} W[+1]_{ecxy} \\in (C[-1] dd, d[+1]d[+1], C[+1])$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = compute_mask_tensor_t((h0, w0), conv1)\n",
    "assert t1.shape == (h1 * w1, hd1 * wd1, h0 * w0)\n",
    "del t1\n",
    "\n",
    "t2 = compute_mask_tensor_t((h1, w1), conv2)\n",
    "assert t2.shape == (h2 * w2, hd2 * wd2, h1 * w1)\n",
    "\n",
    "tt2 = torch.einsum(\"mbl, mdk -> bldk\", t2, t2)\n",
    "assert tt2.shape == (hd2 * wd2, h1 * w1, hd2 * wd2, h1 * w1)\n",
    "\n",
    "old_tt2 = torch.einsum('xkl, xkm->lm', t2, t2)\n",
    "assert old_tt2.shape == (h1 * w1, h1 * w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$T \\in (H[+1]W[+1], d[+1]d[+1], HW)$\n",
    "\n",
    "$(TT)^{bldk} = T^{mbl} T_{mdk} \\in (d[+1]d[+1], HW, d[+1]d[+1], HW)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T X\n",
    "t2x = torch.einsum(\"ial, mbl -> imab\", x_unfolded, t2)\n",
    "assert t2x.shape == (n, h2 * w2, c0 * hd1 * wd1, hd2 * wd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2x_bis = torch.zeros((n, h2 * w2, c0 * hd1 * wd1, hd2 * wd2))\n",
    "\n",
    "assert t2.shape == (h2 * w2, hd2 * wd2, h1 * w1)\n",
    "assert x_unfolded.shape == (n, c0 * hd1 * wd1, h1 * w1)\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(h2 * w2):\n",
    "        t2x_bis[i, j] = x_unfolded[i].detach() @ t2[j].T\n",
    "\n",
    "assert torch.allclose(t2x, t2x_bis, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X^{imab} = \\hat{B}^{ial} T_{mbl} \\in (n, H[+1]W[+1], C[-1]dd, d[+1]d[+1])$\n",
    "\n",
    "$X[i, j] = \\hat{B}[i] \\times T[j]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_f_th = y_th.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "assert y_f_th.shape == (n, h2 * w2, c2), f\"{y_f_th.shape=}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y \\in (n, H[+1]W[+1], C[+1])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_theta = torch.einsum(\"imab, abc -> imc\", t2x, theta)\n",
    "assert x_theta.shape == (n, h2 * w2, c2)\n",
    "\n",
    "assert_close(y_f_th, x_theta, \"The formula is not correct\", atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_theta_2 = torch.einsum(\"ial, mbl, abc -> imc\", x_unfolded, t2, theta)\n",
    "assert x_theta_2.shape == (n, h2 * w2, c2)\n",
    "\n",
    "assert_close(y_f_th, x_theta_2, \"The formula is not correct\", atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29350/3156580705.py:18: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3675.)\n",
      "  x_theta_3[i, m, j] += omega.T @ t2x[i, j].T @ alpha_flat[k]\n"
     ]
    }
   ],
   "source": [
    "x_theta_3 = torch.zeros((n, c2, h2 * w2))\n",
    "\n",
    "assert t2x.shape == (n, h2 * w2, c0 * hd1 * wd1, hd2 * wd2)\n",
    "assert conv2.weight.shape == (c2, c1, hd2, wd2)\n",
    "assert conv1.weight.shape == (c1, c0, hd1, wd1)\n",
    "alpha_flat = conv1.weight.flatten(start_dim=1)\n",
    "assert alpha_flat.shape == (c1, c0 * hd1 * wd1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    for m in range(c2):\n",
    "        for j in range(h2 * w2):\n",
    "            for k in range(c1):\n",
    "                omega = conv2.weight[m, k].flatten()\n",
    "                assert omega.shape == (hd2 * wd2,)\n",
    "                x_theta_3[i, m, j] += omega.T @ t2x[i, j].T @ alpha_flat[k]\n",
    "\n",
    "\n",
    "assert_close(x_theta_3.permute(0, 2, 1), y_f_th, \"The formula is not correct\", atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Conv_2(Conv_1(X)) \\sim X^{imab} \\Theta^{abc} = \\hat{B}^{ial} T_{mbl} \\Theta^{abc}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_f = theta.flatten(0, 1)\n",
    "\n",
    "assert theta_f.shape == (c0 * hd1 * wd1 * hd2 * wd2, c2), f\"{theta_f.shape=}\"\n",
    "x_f = t2x.flatten(0, 1)\n",
    "x_f = x_f.flatten(start_dim=1)\n",
    "assert x_f.shape == (n * h2 * w2, c0 * hd1 * wd1 * hd2 * wd2), f\"{x_f.shape=}\"\n",
    "\n",
    "y_f = x_f @ theta_f\n",
    "assert y_f.shape == (n * h2 * w2, c2)\n",
    "\n",
    "assert_close(y_f, y_f_th.flatten(0, 1), \"The formula is not correct\", atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X_F \\in (n H[+1]W[+1], C[-1]ddd[+1]d[+1])$\n",
    "\n",
    "$\\Theta_F \\in (C[-1]ddd[+1]d[+1], C[+1])$\n",
    "\n",
    "$Y_F \\in (n H[+1]W[+1], C[+1])$ \n",
    "\n",
    "$Conv_2(Conv_1(X)) \\sim X_F \\Theta_F$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the definition with matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S := X_F^T X_F \\in (C[-1]ddd[+1]d[+1], C[-1]ddd[+1]d[+1])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_f = x_f.t() @ x_f\n",
    "assert s_f.shape == (c0 * hd1 * wd1 * hd2 * wd2, c0 * hd1 * wd1 * hd2 * wd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.96 ms ± 212 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = torch.einsum(\"imab, imcd -> abcd\", t2x, t2x)\n",
    "assert s.shape == (c0 * hd1 * wd1, hd2 * wd2, c0 * hd1 * wd1, hd2 * wd2)\n",
    "s = s.flatten(2, 3).flatten(0, 1)\n",
    "assert s.shape == (c0 * hd1 * wd1 * hd2 * wd2, c0 * hd1 * wd1 * hd2 * wd2)\n",
    "assert_close(s, s_f, \"The formula is not correct\", atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.4 ms ± 4.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# WARNING: This is very memory consuming\n",
    "s2 = torch.einsum(\"ial, mbl, ick, mdk -> abcd\", x_unfolded, t2, x_unfolded, t2)\n",
    "assert s2.shape == (c0 * hd1 * wd1, hd2 * wd2, c0 * hd1 * wd1, hd2 * wd2)\n",
    "s2 = s2.flatten(2, 3).flatten(0, 1)\n",
    "assert s2.shape == (c0 * hd1 * wd1 * hd2 * wd2, c0 * hd1 * wd1 * hd2 * wd2)\n",
    "assert_close(s2, s_f, \"The formula is not correct\", atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S^{abcd} = X^{imab} X^{imcd} \\in (C[-1]dd, d[+1]d[+1], C[-1]dd, d[+1]d[+1])$\n",
    "\n",
    "$S^{abcd} = \\hat{B}^{ial} T_{mbl} \\hat{B}^{ick} T_{mdk} \\in (C[-1]dd, d[+1]d[+1], C[-1]dd, d[+1]d[+1])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_unfolded.shape == (n, c0 * hd1 * wd1, h1 * w1)\n",
    "assert old_tt2.shape == (h1 * w1, h1 * w1)\n",
    "old_s = torch.einsum('ial, lm, ibm -> ab', x_unfolded, old_tt2, x_unfolded)\n",
    "assert old_s.shape == (c0 * hd1 * wd1, c0 * hd1 * wd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_unfolded.shape == (n, c0 * hd1 * wd1, h1 * w1)\n",
    "assert t2.shape == (h2 * w2, hd2 * wd2, h1 * w1)\n",
    "b_t = torch.einsum(\"ial, jbl -> ijab\", x_unfolded, t2)\n",
    "assert b_t.shape == (n, h2 * w2, c0 * hd1 * wd1, hd2 * wd2)\n",
    "olds_s2 = torch.einsum('ijab, ijcb -> ac', b_t, b_t)\n",
    "assert olds_s2.shape == (c0 * hd1 * wd1, c0 * hd1 * wd1)\n",
    "assert_close(olds_s2, old_s, \"The formula is not correct\", atol=1e-5, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_close(olds_s2, old_s, \"The formula is not correct\", atol=1e-5, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_s3 = torch.einsum('ial, jbl, icl, jbl -> ac', x_unfolded, t2, x_unfolded, t2)\n",
    "assert old_s3.shape == (c0 * hd1 * wd1, c0 * hd1 * wd1)\n",
    "\n",
    "assert_close(old_s3, old_s, \"The formula is not correct\", atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$B^t_{i, j} := T_j (B^c_i)^T$ or $(B^t)^{ij} = T^{jbl} (B^c)^{ial}$\n",
    "\n",
    "$S := \\sum_{i = 1}^n \\sum_{j = 1}^{H[+1]W[+1]} (B^t_{i, j})^T (B^t_{i, j}) \\in (C[-1] dd, C[-1] dd) = (B^t)^{ijab} (B^t)^{ijcb}$\n",
    "\n",
    "$S^{ac} = (B^c)^{ial} T_{jbl} (B^c)^{icl} T_{jbl} \\in (C[-1] dd, C[-1] dd)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute N\n",
    "(named M in the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N := X_F^T Y_F \\in (C[-1]ddd[+1]d[+1], C[+1])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_f = x_f.t() @ y_f\n",
    "assert m_f.shape == (c0 * hd1 * wd1 * hd2 * wd2, c2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_f_th.shape == (n, h2 * w2, c2)\n",
    "assert t2x.shape == (n, h2 * w2, c0 * hd1 * wd1, hd2 * wd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.einsum(\"ixab, ixc -> abc\", t2x, y_f_th)\n",
    "assert m.shape == (c0 * hd1 * wd1, hd2 * wd2, c2)\n",
    "m = m.flatten(0, 1)\n",
    "assert m.shape == (c0 * hd1 * wd1 * hd2 * wd2, c2)\n",
    "assert_close(m, m_f, \"The formula is not correct\", atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = torch.einsum(\"ial, xbl, ixc -> abc\", x_unfolded, t2, y_f_th)\n",
    "assert m2.shape == (c0 * hd1 * wd1, hd2 * wd2, c2)\n",
    "m2 = m2.flatten(0, 1)\n",
    "assert m2.shape == (c0 * hd1 * wd1 * hd2 * wd2, c2)\n",
    "assert_close(m2, m_f, \"The formula is not correct\", atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N^{abc} = X^{ixab} Y^{ixc} \\in (C[-1]dd, d[+1]d[+1], C[+1])$\n",
    "\n",
    "$N^{abc} = \\hat{B}^{ial} T_{xbl} \\Theta^{ixc} \\in (C[-1]dd, d[+1]d[+1], C[+1])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert t2.shape == (h2 * w2, hd2 * wd2, h1 * w1)\n",
    "assert x_unfolded.shape == (n, c0 * hd1 * wd1, h1 * w1)\n",
    "assert y_f_th.shape == (n, h2 * w2, c2)\n",
    "\n",
    "old_n = torch.einsum('xkl, ial, ixc -> ack', t2, x_unfolded, y_f_th)\n",
    "\n",
    "assert old_n.shape == (c0 * hd1 * wd1, c2, hd2 * wd2)\n",
    "old_n = old_n.flatten(1, 2)\n",
    "assert old_n.shape == (c0 * hd1 * wd1, c2 * hd2 * wd2)\n",
    "# x : h2 * w2\n",
    "# k: hd2 * wd2\n",
    "# l: h1 * w1\n",
    "# i: n\n",
    "# a: c0 * hd1 * wd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert b_t.shape == (n, h2 * w2, c0 * hd1 * wd1, hd2 * wd2)\n",
    "old_n2 = torch.einsum('ijm, ijak -> amk', y_f_th, b_t)\n",
    "assert old_n2.shape == (c0 * hd1 * wd1, c2, hd2 * wd2)\n",
    "assert_close(old_n2.flatten(1, 2), old_n, \"The formula is not correct\", atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add optimal neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New S, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1125, 7]), 7875)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = s @ m\n",
    "assert sm.shape == (c0 * hd1 * wd1 * hd2 * wd2, c2)\n",
    "sm.shape, sm.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trudkiew/Documents/GrowingNetwork/GroMo/src/gromo/tools.py:97: UserWarning: Warning: The input matrix S is not symmetric.\n",
      "Max difference: 1.91e-06,% of non-zero elements: 2.41%\n",
      "  warn(\n",
      "[W Context.cpp:258] Warning: torch.backends.cuda.preferred_linalg_library is an experimental feature. If you see any error or unexpected behavior when this flag is set please file an issue on GitHub. (function operator())\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m k \u001b[38;5;241m=\u001b[39m sigma\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mmin\u001b[39m(c0 \u001b[38;5;241m*\u001b[39m hd1 \u001b[38;5;241m*\u001b[39m wd1 \u001b[38;5;241m*\u001b[39m hd2 \u001b[38;5;241m*\u001b[39m wd2, c2)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m alpha\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (c0 \u001b[38;5;241m*\u001b[39m hd1 \u001b[38;5;241m*\u001b[39m wd1 \u001b[38;5;241m*\u001b[39m hd2 \u001b[38;5;241m*\u001b[39m wd2, k)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m omega\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (k, c2)\n\u001b[1;32m      6\u001b[0m sigma\u001b[38;5;241m.\u001b[39mshape, sigma\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha, omega, sigma = compute_optimal_added_parameters(s, m, numerical_threshold=1e-15, statistical_threshold=1e-15)\n",
    "k = sigma.shape[0]\n",
    "assert k == min(c0 * hd1 * wd1 * hd2 * wd2, c2)\n",
    "assert alpha.shape == (c0 * hd1 * wd1 * hd2 * wd2, k)\n",
    "assert omega.shape == (k, c2)\n",
    "sigma.shape, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_from_alpha_omega(alpha: torch.Tensor, \n",
    "                          omega: torch.Tensor, \n",
    "                          conv1: torch.nn.Conv2d, \n",
    "                          conv2: torch.nn.Conv2d,\n",
    "                          k: int) -> tuple[torch.nn.Conv2d, torch.nn.Conv2d]:\n",
    "    \"\"\"\n",
    "    Create a convolutional layer from the alpha and omega parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha: torch.Tensor\n",
    "        tensor alpha in (c0 * hd1 * wd1 * hd2 * wd2, k)\n",
    "    omega: torch.Tensor\n",
    "        tensor omega in (k, c2)\n",
    "    conv1: torch.nn.Conv2d\n",
    "        convolutional layer 1\n",
    "    conv2: torch.nn.Conv2d\n",
    "        convolutional layer 2\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Conv2d\n",
    "        convolutional layer\n",
    "    \"\"\"\n",
    "    c1, c0, hd1, wd1 = conv1.weight.shape\n",
    "    c2, _, hd2, wd2 = conv2.weight.shape\n",
    "    assert c1 == _, f\"{c1=} != {_=}\"\n",
    "    max_k = min(c0 * hd1 * wd1 * hd2 * wd2, c2)\n",
    "    assert alpha.shape == (c0 * hd1 * wd1 * hd2 * wd2, max_k), f\"{alpha.shape=}\"\n",
    "    assert omega.shape == (max_k, c2)\n",
    "    k = min(max_k, k)\n",
    "\n",
    "    theta = alpha @ omega\n",
    "    assert theta.shape == (c0 * hd1 * wd1 * hd2 * wd2, c2)\n",
    "    theta = theta.reshape((c0 * hd1 * wd1, hd2 * wd2, c2)).permute(0, 2, 1).flatten(1, 2)\n",
    "    assert theta.shape == (c0 * hd1 * wd1, c2 * hd2 * wd2)\n",
    "\n",
    "    u, s, v = torch.linalg.svd(theta, full_matrices=False)\n",
    "    assert (torch.all(s >= 0))\n",
    "    s = torch.sqrt(s[:k])\n",
    "    u = u[:, :k] * s\n",
    "    v = v[:k, :] * s.unsqueeze(1)\n",
    "    \n",
    "    new_conv1 = torch.nn.Conv2d(c0, k, (hd1, wd1), bias=False)\n",
    "    new_conv1.weight.data = u.reshape((k, c0, hd1, wd1))\n",
    "    new_conv2 = torch.nn.Conv2d(k, c2, (hd2, wd2), bias=False)\n",
    "    new_conv2.weight.data = v.reshape((c2, k, hd2, wd2))\n",
    "\n",
    "    return new_conv1, new_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1295.1821, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_conv1, new_conv2 = conv_from_alpha_omega(alpha, omega, conv1, conv2, k)\n",
    "\n",
    "new_y = new_conv2(new_conv1(x))\n",
    "torch.norm(new_y - y_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old S, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([75, 105]), 7875)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sn_old = old_s @ old_n\n",
    "assert sn_old.shape == (c0 * hd1 * wd1, c2 * hd2 * wd2)\n",
    "sn_old.shape, sn_old.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([75]),\n",
       " tensor([20.3776, 17.9205, 17.6068, 17.3225, 17.0303, 16.3034, 15.2867, 15.2516,\n",
       "         13.6711, 12.6789, 12.1093, 11.8605, 11.6597, 11.2274, 10.5018, 10.2002,\n",
       "          9.9270,  9.7805,  9.5504,  8.8780,  8.5152,  8.1829,  8.0237,  7.9264,\n",
       "          7.7054,  7.2579,  7.1677,  6.8433,  6.6986,  6.4866,  6.4120,  6.0317,\n",
       "          5.9407,  5.8310,  5.5817,  5.3840,  5.2577,  5.1800,  5.0048,  4.7238,\n",
       "          4.6380,  4.3988,  4.3635,  4.1891,  4.0053,  3.8942,  3.7571,  3.6106,\n",
       "          3.5652,  3.5043,  3.3150,  3.1713,  3.0611,  2.9485,  2.6393,  2.5713,\n",
       "          2.5231,  2.4216,  2.3493,  2.2053,  2.0556,  1.8755,  1.8494,  1.7431,\n",
       "          1.6724,  1.6123,  1.5173,  1.3643,  1.2467,  1.2000,  1.0787,  0.9900,\n",
       "          0.8586,  0.7888,  0.6594], grad_fn=<IndexBackward0>))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_alpha, old_omega, sigma = compute_optimal_added_parameters(old_s, old_n, numerical_threshold=1e-15, statistical_threshold=1e-15)\n",
    "k = sigma.shape[0]\n",
    "assert k == min(c2 * hd2 * wd2, c0 * hd1 * wd1), f\"{k=}\"\n",
    "assert old_alpha.shape == (c0 * hd1 * wd1, k), f\"{old_alpha.shape=}\"\n",
    "assert old_omega.shape == (k, c2 * hd2 * wd2)\n",
    "sigma.shape, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_from_alpha_omega_old(alpha: torch.Tensor, \n",
    "                              omega: torch.Tensor, \n",
    "                              conv1: torch.nn.Conv2d, \n",
    "                              conv2: torch.nn.Conv2d,\n",
    "                              k: int) -> tuple[torch.nn.Conv2d, torch.nn.Conv2d]:\n",
    "    \"\"\"\n",
    "    Create a convolutional layer from the alpha and omega parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha: torch.Tensor\n",
    "        tensor alpha in (c0 * hd1 * wd1 * hd2 * wd2, k)\n",
    "    omega: torch.Tensor\n",
    "        tensor omega in (k, c2)\n",
    "    conv1: torch.nn.Conv2d\n",
    "        convolutional layer 1\n",
    "    conv2: torch.nn.Conv2d\n",
    "        convolutional layer 2\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Conv2d\n",
    "        convolutional layer\n",
    "    \"\"\"\n",
    "    c1, c0, hd1, wd1 = conv1.weight.shape\n",
    "    c2, _, hd2, wd2 = conv2.weight.shape\n",
    "    assert c1 == _, f\"{c1=} != {_=}\"\n",
    "    max_k = min(c0 * hd1 * wd1, c2 * hd2 * wd2)\n",
    "    assert alpha.shape == (c0 * hd1 * wd1, max_k), f\"{alpha.shape=}\"\n",
    "    assert omega.shape == (max_k, c2 * hd2 * wd2)\n",
    "    k = min(k, max_k)\n",
    "\n",
    "    alpha = alpha[:, :k]\n",
    "    omega = omega[:k, :]\n",
    "\n",
    "    new_conv1 = torch.nn.Conv2d(c0, k, (hd1, wd1), bias=False)\n",
    "    new_conv1.weight.data = alpha.reshape((k, c0, hd1, wd1))\n",
    "\n",
    "    new_conv2 = torch.nn.Conv2d(k, c2, (hd2, wd2), bias=False)\n",
    "    new_conv2.weight.data = omega.reshape((c2, k, hd2, wd2))\n",
    "\n",
    "    return new_conv1, new_conv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(99.2078, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_conv1_old, new_conv2_old = conv_from_alpha_omega_old(old_alpha, old_omega, conv1, conv2, k)\n",
    "\n",
    "new_y_old = new_conv2_old(new_conv1_old(x))\n",
    "torch.norm(new_y_old - y_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA_3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
