import torch.nn
from torch import nn
from torch.nn import functional as F

from gromo.linear_growing_module import LinearGrowingModule


class GrowingMLPBlock(nn.Module):
    """
    Represents a block of a growing network.

    Sequence of layers:
    - Activation pre
    - Layer first
    - Activation mid
    - Layer second
    """

    def __init__(
            self,
            num_features: int,
            hidden_features: int = 0,
            dropout: float = 0.0,
            name: str | None = None,
            kwargs_layer: dict | None = None,
    ) -> None:
        """
        Initialise the block.

        Parameters
        ----------
        in_out_features: int
            number of input and output features, in cas of convolutional layer, the number of channels
        hidden_features: int
            number of hidden features, if zero the block is the zero function
        layer_type: str
            type of layer to use either "linear" or "conv"
        activation: torch.nn.Module | None
            activation function to use, if None use the identity function
        name: str
            name of the block
        kwargs_layer: dict | None
            dictionary of arguments for the layers (e.g. bias, ...)
        """
        if kwargs_layer is None:
            kwargs_layer = {}

        super(GrowingMLPBlock, self).__init__()
        self.name = name

        self.norm = nn.LayerNorm(num_features)
        self.first_layer = LinearGrowingModule(
            num_features,
            hidden_features,
            post_layer_function=torch.nn.GELU(),
            name=f"{name}: first layer",
            **kwargs_layer,
        )
        self.dropout = nn.Dropout(dropout)
        self.second_layer = LinearGrowingModule(
            hidden_features,
            num_features,
            post_layer_function=nn.Identity(),
            previous_module=self.first_layer,
            name=f"{name}: second layer",
            **kwargs_layer,
        )
        self.enable_extended_forward = False

        # self.activation_derivative = torch.func.grad(self.first_layer.post_layer_function)(torch.tensor(1e-5))
        # TODO: FIX this
        # self.activation_derivative = 1

    def __setattr__(self, key, value):
        if key in ["scaling_factor", "eigenvalues_extension", "parameter_update_decrease", "first_order_improvement"]:
            self.second_layer.__setattr__(key, value)
        else:
            nn.Module.__setattr__(self, key, value)

    @property
    def hidden_features(self):
        return self.second_layer.in_features

    @property
    def scaling_factor(self):
        return self.second_layer.scaling_factor

    @property
    def eigenvalues_extension(self):
        return self.second_layer.eigenvalues_extension

    @property
    def parameter_update_decrease(self):
        return self.second_layer.parameter_update_decrease

    @property
    def first_order_improvement(self) -> torch.Tensor:
        """
        Get the first order improvement of the block.

        Returns
        -------
        torch.Tensor
            first order improvement
        """
        return self.second_layer.first_order_improvement

    def extended_forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the block with the current modifications.

        Parameters
        ----------
        x: torch.Tensor
            input tensor

        Returns
        -------
        torch.Tensor
            output tensor
        """
        y, y_ext = self.first_layer.extended_forward(x)
        y = self.dropout(y)
        if y_ext is not None:
            y_ext = self.dropout(y_ext)
        y, _ = self.second_layer.extended_forward(y, y_ext)

        assert (
                _ is None
        ), f"The output of layer 2 {self.second_layer.name} should not be extended."
        del y_ext

        y = self.dropout(y)
        return y

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the block.

        Parameters
        ----------
        x: torch.Tensor
            input tensor

        Returns
        -------
        torch.Tensor
            output tensor
        """
        y = self.first_layer(x)
        y = self.dropout(y)
        y = self.second_layer(y)
        y = self.dropout(y)
        return y

    @property
    def in_activity(self) -> torch.Tensor:
        """
        Get the input activity of the block.

        Returns
        -------
        torch.Tensor
            input activity
        """
        return self.first_layer.input

    def set_store_in_activity(self, value: bool):
        """
        Set the store_in_activity parameter of the block.
        If True, the block will store the activity after the first activation
        function.

        Parameters
        ----------
        value: bool
            value to set
        """
        self.first_layer.store_input = True

    def init_computation(self):
        """
        Initialise the computation of the block.
        """
        # self.first_layer.init_computation()
        self.second_layer.init_computation()

    def update_computation(self, desired_activation: torch.Tensor | None = None):
        """
        Update the computation of the block.

        Parameters
        ----------
        desired_activation: torch.Tensor
            desired direction of the output variation of the block
        """
        # self.first_layer.update_computation()
        self.second_layer.update_computation()

    def reset_computation(self):
        """
        Reset the computation of the block.
        """
        self.first_layer.reset_computation()
        self.second_layer.reset_computation()

    def delete_update(self):
        """
        Delete the update of the block.
        """
        self.second_layer.delete_update(include_previous=True)

    def compute_optimal_update(
            self,
            part: str = "all",
            numerical_threshold: float = 1e-15,
            statistical_threshold: float = 1e-3,
            maximum_added_neurons: int | None = None,
            dtype: torch.dtype = torch.float32,
    ) -> None:
        """
        Compute the optimal update for second layer and additional neurons.

        Parameters
        ----------
        numerical_threshold: float
            threshold to consider an eigenvalue as zero in the square root of the inverse of S
        statistical_threshold: float
            threshold to consider an eigenvalue as zero in the SVD of S{-1/2} N
        maximum_added_neurons: int | None
            maximum number of added neurons, if None all significant neurons are kept
        """
        assert part in [
            "all",
            "parameter",
            "neuron",
        ], f"{part=} should be in ['all', 'parameter', 'neuron']"

        if part == "parameter":
            _, _, self.parameter_update_decrease = self.second_layer.compute_optimal_delta(dtype=dtype)
            # _, _, _ = self.second_layer.compute_optimal_delta(dtype=dtype)
        elif part == "neuron":
            _, _ = self.second_layer.compute_optimal_updates(
                zero_delta=True,
                numerical_threshold=numerical_threshold,
                statistical_threshold=statistical_threshold,
                maximum_added_neurons=maximum_added_neurons,
                dtype=dtype,
            )
            self.second_layer.optimal_delta_layer = None
            self.second.layer.parameter_update_decrease = 0
        elif part == "all":
            _, _ = (
                self.second_layer.compute_optimal_updates(
                    numerical_threshold=numerical_threshold,
                    statistical_threshold=statistical_threshold,
                    maximum_added_neurons=maximum_added_neurons,
                    update_previous=True,
                    dtype=dtype,
                )
            )

    def apply_change(self) -> None:
        """
        Apply the optimal delta and extend the layer with current
        optimal delta and layer extension with the current scaling factor.
        """
        self.second_layer.apply_change(apply_previous=True)
        # self.hidden_features += self.eigenvalues_extension.shape[0]

    def sub_select_optimal_added_parameters(
            self,
            keep_neurons: int,
    ) -> None:
        """
        Select the first keep_neurons neurons of the optimal added parameters.

        Parameters
        ----------
        keep_neurons: int
            number of neurons to keep
        """
        self.eigenvalues = self.eigenvalues[:keep_neurons]
        self.second_layer.sub_select_optimal_added_parameters(keep_neurons, sub_select_previous=True)

    def number_of_parameters(self):
        num_param = self.first_layer.number_of_parameters()
        num_param += self.second_layer.number_of_parameters()
        return num_param

    @staticmethod
    def tensor_statistics(tensor) -> dict[str, float]:
        min_value = tensor.min().item()
        max_value = tensor.max().item()
        mean_value = tensor.mean().item()
        if tensor.numel() > 1:
            std_value = tensor.std().item()
        else:
            std_value = -1
        return {
            "min": min_value,
            "max": max_value,
            "mean": mean_value,
            "std": std_value,
        }

    def weights_statistics(self) -> dict[int, dict[str, dict[str, float] | int]]:
        statistics = {}
        for i, layer in enumerate([self.first_layer, self.second_layer]):
            statistics[i] = {
                "weight": self.tensor_statistics(layer.weight),
            }
            if layer.bias is not None:
                statistics[i]["bias"] = self.tensor_statistics(layer.bias)
            statistics[i]["input_shape"] = layer.in_features
            statistics[i]["output_shape"] = layer.out_features
        return statistics

    def update_information(self):
        layer_information = dict()
        layer_information["update_value"] = self.first_order_improvement
        layer_information["parameter_improvement"] = self.parameter_update_decrease
        layer_information["eigenvalues_extension"] = self.eigenvalues_extension
        return layer_information


__growing_methods__ = [
    "init_computation",
    "reset_computation",
    "update_computation",
    "delete_update",
    "compute_optimal_update",
    "apply_change",
    "number_of_parameters",
    "weights_statistics",
    "update_information",
]

__growing_attributes__ = [
    "scaling_factor",
    "eigenvalues_extension",
    "parameter_update_decrease",
    "first_order_improvement"
]


class GrowingTokenMixer(nn.Module):
    def __init__(self, num_patches, num_features, hidden_features, dropout, name=None):
        super(GrowingTokenMixer, self).__init__()
        self.norm = nn.LayerNorm(num_features)
        self.mlp = GrowingMLPBlock(num_patches, hidden_features, dropout, name=f"{name}: Token Mixer")
        for item in __growing_methods__:
            setattr(self, item, getattr(self.mlp, item))

    def __getattr__(self, item):
        if item in __growing_attributes__:
            return getattr(self.mlp, item)
        else:
            return super(GrowingTokenMixer, self).__getattr__(item)

    def __setattr__(self, key, value):
        if key in __growing_attributes__:
            self.mlp.__setattr__(key, value)
        else:
            nn.Module.__setattr__(self, key, value)

    def forward(self, x):
        # x.shape == (batch_size, num_patches, num_features)
        residual = x
        x = self.norm(x)
        x = x.transpose(1, 2)
        # x.shape == (batch_size, num_features, num_patches)
        x = self.mlp(x)
        x = x.transpose(1, 2)
        # x.shape == (batch_size, num_patches, num_features)
        out = x + residual
        return out

    def extended_forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the block with the current modifications.

        Parameters
        ----------
        x: torch.Tensor
            input tensor

        Returns
        -------
        torch.Tensor
            output tensor
        """
        residual = x
        y = self.norm(x)
        y = y.transpose(1, 2)
        y = self.mlp.extended_forward(y)
        y = y.transpose(1, 2)
        out = y + residual
        return out


class GrowingChannelMixer(nn.Module):
    def __init__(self, num_features, hidden_features, dropout, name=None):
        super(GrowingChannelMixer, self).__init__()
        self.norm = nn.LayerNorm(num_features)
        self.mlp = GrowingMLPBlock(num_features, hidden_features, dropout, name=f"{name}: Channel Mixer")
        for item in __growing_methods__:
            setattr(self, item, getattr(self.mlp, item))

    def __getattr__(self, item):
        if item in __growing_attributes__:
            return getattr(self.mlp, item)
        else:
            return super(GrowingChannelMixer, self).__getattr__(item)

    def forward(self, x):
        # x.shape == (batch_size, num_patches, num_features)
        residual = x
        x = self.norm(x)
        x = self.mlp(x)
        # x.shape == (batch_size, num_patches, num_features)
        out = x + residual
        return out

    def extended_forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the block with the current modifications.

        Parameters
        ----------
        x: torch.Tensor
            input tensor

        Returns
        -------
        torch.Tensor
            output tensor
        """
        residual = x
        x = self.norm(x)
        x = self.mlp.extended_forward(x)
        # x.shape == (batch_size, num_patches, num_features)
        out = x + residual
        return out


class GrowingMixerLayer(nn.Module):
    def __init__(self, num_patches, num_features, hidden_features, dropout, name="Mixer Layer"):
        super(GrowingMixerLayer, self).__init__()
        self.token_mixer = GrowingTokenMixer(
            num_patches, num_features, hidden_features, dropout, name=f"{name}"
        )
        self.channel_mixer = GrowingChannelMixer(
            num_features, hidden_features, dropout, name=f"{name}"
        )

    def forward(self, x):
        # x.shape == (batch_size, num_patches, num_features)
        x = self.token_mixer(x)
        x = self.channel_mixer(x)
        # x.shape == (batch_size, num_patches, num_features)
        return x

    def extended_forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the block with the current modifications.

        Parameters
        ----------
        x: torch.Tensor
            input tensor

        Returns
        -------
        torch.Tensor
            output tensor
        """
        x = self.token_mixer.extended_forward(x)
        x = self.channel_mixer.extended_forward(x)
        return x

    def init_computation(self):
        """
        Initialise the computation of the block.
        """
        self.token_mixer.init_computation()
        self.channel_mixer.init_computation()

    def reset_computation(self):
        """
        Reset the computation of the block.
        """
        self.token_mixer.reset_computation()
        self.channel_mixer.reset_computation()

    def update_computation(self):
        """
        Update the computation of the block.
        """
        self.token_mixer.update_computation()
        self.channel_mixer.update_computation()

    def delete_update(self):
        """
        Delete the update of the block.
        """
        self.token_mixer.delete_update()
        self.channel_mixer.delete_update()

    def compute_optimal_update(
            self,
            part: str = "all",
            numerical_threshold: float = 1e-15,
            statistical_threshold: float = 1e-3,
            maximum_added_neurons: int | None = None,
            dtype: torch.dtype = torch.float32,
    ) -> None:
        """
        Compute the optimal update for second layer and additional neurons.

        Parameters
        ----------
        numerical_threshold: float
            threshold to consider an eigenvalue as zero in the square root of the inverse of S
        statistical_threshold: float
            threshold to consider an eigenvalue as zero in the SVD of S{-1/2} N
        maximum_added_neurons: int | None
            maximum number of added neurons, if None all significant neurons are kept
        """
        self.token_mixer.compute_optimal_update(
            part=part,
            numerical_threshold=numerical_threshold,
            statistical_threshold=statistical_threshold,
            maximum_added_neurons=maximum_added_neurons,
            dtype=dtype,
        )
        self.channel_mixer.compute_optimal_update(
            part=part,
            numerical_threshold=numerical_threshold,
            statistical_threshold=statistical_threshold,
            maximum_added_neurons=maximum_added_neurons,
            dtype=dtype,
        )

    def apply_change(self) -> None:
        """
        Apply the optimal delta and extend the layer with current
        optimal delta and layer extension with the current scaling factor.
        """
        self.token_mixer.apply_change()
        self.channel_mixer.apply_change()

    def number_of_parameters(self):
        num_param = self.token_mixer.number_of_parameters()
        num_param += self.channel_mixer.number_of_parameters()
        return num_param

    def weights_statistics(self) -> dict[int, dict[str, dict[str, float] | int]]:
        statistics = {}
        statistics[0] = self.token_mixer.weights_statistics()
        statistics[1] = self.channel_mixer.weights_statistics()
        return statistics

    def update_information(self):
        layer_information = dict()
        layer_information["token_mixer"] = self.token_mixer.update_information()
        layer_information["channel_mixer"] = self.channel_mixer.update_information()
        return layer_information


def check_sizes(image_size, patch_size):
    sqrt_num_patches, remainder = divmod(image_size, patch_size)
    assert remainder == 0, "`image_size` must be divisibe by `patch_size`"
    num_patches = sqrt_num_patches ** 2
    return num_patches


class GrowingMLPMixer(nn.Module):
    def __init__(
            self,
            image_size=256,
            patch_size=16,
            in_channels=3,
            num_features=128,
            hidden_features=2,
            num_layers=8,
            num_classes=10,
            dropout=0.5,
    ):
        num_patches = check_sizes(image_size, patch_size)
        super(GrowingMLPMixer, self).__init__()
        # per-patch fully-connected is equivalent to strided conv2d
        print(f"in_channels: {in_channels}, num_features: {num_features}, patch_size: {patch_size}")
        self.patcher = nn.Conv2d(
            in_channels, num_features, kernel_size=patch_size, stride=patch_size
        )
        self.mixers: nn.ModuleList[GrowingMixerLayer] = nn.ModuleList(
            [
                GrowingMixerLayer(num_patches, num_features, hidden_features, dropout, name=f"Layer {_}")
                for _ in range(num_layers)
            ]
        )
        self.classifier = nn.Linear(num_features, num_classes)
        self.currently_updated_block = None

    def forward(self, x):
        patches = self.patcher(x)
        batch_size, num_features, _, _ = patches.shape
        patches = patches.permute(0, 2, 3, 1)
        patches = patches.view(batch_size, -1, num_features)
        # patches.shape == (batch_size, num_patches, num_features)
        embedding = patches
        for mixer in self.mixers:
            embedding = mixer(embedding)
        # embedding.shape == (batch_size, num_patches, num_features)
        embedding = embedding.mean(dim=1)
        logits = self.classifier(embedding)
        return logits

    def extended_forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the block with the current modifications.

        Parameters
        ----------
        x: torch.Tensor
            input tensor

        Returns
        -------
        torch.Tensor
            output tensor
        """
        patches = self.patcher(x)
        batch_size, num_features, _, _ = patches.shape
        patches = patches.permute(0, 2, 3, 1)
        patches = patches.view(batch_size, -1, num_features)
        # patches.shape == (batch_size, num_patches, num_features)
        embedding = patches
        for mixer in self.mixers:
            embedding = mixer.extended_forward(embedding)
        # embedding.shape == (batch_size, num_patches, num_features)
        embedding = embedding.mean(dim=1)
        logits = self.classifier(embedding)
        return logits

    def init_computation(self):
        """
        Initialise the computation of the block.
        """
        for mixer in self.mixers:
            mixer.init_computation()

    def reset_computation(self):
        """
        Reset the computation of the block.
        """
        for mixer in self.mixers:
            mixer.reset_computation()

    def update_computation(self):
        """
        Update the computation of the block.
        """
        for mixer in self.mixers:
            mixer.update_computation()

    def delete_update(self):
        """
        Delete the update of the block.
        """
        for mixer in self.mixers:
            mixer.delete_update()
        self.currently_updated_block = None

    def compute_optimal_update(
            self,
            part: str = "all",
            numerical_threshold: float = 1e-15,
            statistical_threshold: float = 1e-3,
            maximum_added_neurons: int | None = None,
            dtype: torch.dtype = torch.float32,
    ) -> None:
        """
        Compute the optimal update for second layer and additional neurons.

        Parameters
        ----------
        numerical_threshold: float
            threshold to consider an eigenvalue as zero in the square root of the inverse of S
        statistical_threshold: float
            threshold to consider an eigenvalue as zero in the SVD of S{-1/2} N
        maximum_added_neurons: int | None
            maximum number of added neurons, if None all significant neurons are kept
        """
        for mixer in self.mixers:
            mixer.compute_optimal_update(
                part=part,
                numerical_threshold=numerical_threshold,
                statistical_threshold=statistical_threshold,
                maximum_added_neurons=maximum_added_neurons,
                dtype=dtype,
            )

    def apply_change(self) -> None:
        """
        Apply the optimal delta and extend the layer with current
        optimal delta and layer extension with the current scaling factor.
        """
        for mixer in self.mixers:
            mixer.apply_change()
        self.currently_updated_block = None

    def number_of_parameters(self):
        num_param = 0
        for mixer in self.mixers:
            num_param += mixer.number_of_parameters()
        return num_param

    def weights_statistics(self) -> dict[int, dict[str, dict[str, float] | int]]:
        statistics = {}
        for i, mixer in enumerate(self.mixers):
            statistics[i] = mixer.weights_statistics()
        return statistics

    def update_information(self):
        layer_information = dict()
        for i, mixer in enumerate(self.mixers):
            layer_information[i] = mixer.update_information()
        return layer_information

    def select_best_update(self):
        token_mixers_first_order_improvement = torch.tensor([
            mixer.token_mixer.first_order_improvement for mixer in self.mixers
        ])
        channel_mixers_first_order_improvement = torch.tensor([
            mixer.channel_mixer.first_order_improvement for mixer in self.mixers
        ])
        best_token_mixer_index = torch.argmax(token_mixers_first_order_improvement)
        best_channel_mixer_index = torch.argmax(channel_mixers_first_order_improvement)

        best_token_mixer_improvement = token_mixers_first_order_improvement[best_token_mixer_index]
        best_channel_mixer_improvement = channel_mixers_first_order_improvement[best_channel_mixer_index]
        token_or_channels = "token" if best_token_mixer_improvement > best_channel_mixer_improvement else "channel"

        for i, mixer in enumerate(self.mixers):
            if token_or_channels == "token":
                if i != best_token_mixer_index:
                    mixer.delete_update()
                else:
                    mixer.channel_mixer.delete_update()
                    self.currently_updated_block = mixer.token_mixer
                    print(f"Selected token mixer {i}")
            elif token_or_channels == "channel":
                if i != best_channel_mixer_index:
                    mixer.delete_update()
                else:
                    mixer.token_mixer.delete_update()
                    self.currently_updated_block = mixer.channel_mixer
                    print(f"Selected channel mixer {i}")

    @property
    def first_order_improvement(self) -> torch.Tensor:
        """
        Get the first order improvement of the block.

        Returns
        -------
        torch.Tensor
            first order improvement
        """
        return self.currently_updated_block.first_order_improvement


