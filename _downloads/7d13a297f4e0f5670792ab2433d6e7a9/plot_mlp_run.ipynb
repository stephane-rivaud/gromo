{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# MLP Example with Growing Layers\n\nThis example shows how to train a MLP with growing layers on sin data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Theo Rudkiewicz <theo.rudkiewicz@inria.fr>\n#          Sylvain Chevallier <sylvain.chevallier@universite-paris-saclay.fr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\nImporting the modules and verifying gpu availability.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom helpers.auxilliary_functions import *\n\nfrom gromo.growing_block import GrowingBlock, LinearGrowingBlock, LinearGrowingModule\nfrom gromo.growing_mlp import GrowingMLP\n\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Auxiliary functions\nWe define some auxiliary functions to train the model, and plot the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class IdDataloader:\n    def __init__(\n        self, nb_sample: int = 1, batch_size: int = 100, seed: int = 0, device=DEVICE\n    ):\n        self.nb_sample = nb_sample\n        self.batch_size = batch_size\n        self.seed = seed\n        self.sample_index = 0\n        self.device = device\n\n    def __iter__(self):\n        torch.manual_seed(self.seed)\n        self.sample_index = 0\n        return self\n\n    def __next__(self):\n        if self.sample_index >= self.nb_sample:\n            raise StopIteration\n        self.sample_index += 1\n        x = torch.rand(self.batch_size, 2, device=self.device)\n        return x, x\n\n\ndef plt_model(model, fig):\n    x = torch.linspace(0, 2 * np.pi, 1000, device=DEVICE).view(-1, 1)\n    y = torch.sin(x)\n    y_pred = model(x)\n    fig.plot(x.cpu().numpy(), y.cpu().numpy(), label=\"sin\")\n    fig.plot(x.cpu().numpy(), y_pred.cpu().detach().numpy(), label=\"Predicted\")\n    for i in range(model[0].bias.data.shape[0]):\n        split = -model[0].bias.data[i] / model[0].weight.data[i, 0]\n        if 0 <= split <= 2 * np.pi:\n            fig.axvline(split.item(), color=\"black\", linestyle=\"--\")\n    fig.legend()\n    fig.set_xlabel(\"x\")\n    fig.yaxis.set_label_position(\"right\")\n    fig.set_ylabel(\"sin(x)\")\n\n\ndef plt_model_id(model, fig):\n    x = torch.linspace(0, 1, 1000, device=DEVICE).view(-1, 1)\n    # y = torch.selu(x)\n    y = x\n    y_pred = model(torch.cat([x, x], dim=1))\n    fig.plot(x.cpu().numpy(), y.cpu().numpy(), label=\"selu\")\n    fig.plot(x.cpu().numpy(), y_pred[:, 0].cpu().detach().numpy(), label=\"Predicted 1\")\n    fig.plot(x.cpu().numpy(), y_pred[:, 1].cpu().detach().numpy(), label=\"Predicted 2\")\n    fig.legend()\n    fig.set_xlabel(\"x\")\n    fig.yaxis.set_label_position(\"right\")\n    fig.set_ylabel(\"selu(x)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handcrafted sin model\nWe define a simple MLP model to approximate the sin function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_neurons = 5\ntorch.manual_seed(0)\nl1 = nn.Linear(1, n_neurons, device=DEVICE)\nl2 = nn.Linear(n_neurons, 1, device=DEVICE)\nnet = nn.Sequential(l1, nn.ReLU(), l2)\n\nbatch_size = 1_000\nnb_sample = 1_000\n\nl1.weight.data = torch.ones_like(l1.weight.data)\na = 1.1\nb = 0.87\nl1.bias.data = -torch.tensor([0, a, np.pi - a, np.pi + a, 2 * np.pi - a], device=DEVICE)\nl2.weight.data = torch.tensor([[b, -b, -b, b, b]], device=DEVICE)\nl2.bias.data = torch.tensor([0.0], device=DEVICE)\n\nl2_err = evaluate_model(\n    net, SinDataloader(nb_sample=nb_sample, batch_size=batch_size), AxisMSELoss()\n)[0]\nprint(f\"Initial error: {l2_err:.2e}\")\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\nplt_model(net, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the network\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\n# optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n\nres = train(\n    net,\n    train_dataloader=SinDataloader(nb_sample=nb_sample, batch_size=batch_size),\n    optimizer=optimizer,\n    nb_epoch=15,\n    show=True,\n)\nloss_train, accuracy_train, loss_val, accuracy_val = res\nplt.plot(loss_train, label=\"train\")\nplt.plot(loss_val, label=\"val\")\nplt.legend()\nplt.show()\n\nplt.plot(accuracy_train, label=\"train\")\nplt.plot(accuracy_val, label=\"val\")\nplt.legend()\nplt.show()\n\nl2_err = evaluate_model(\n    net, SinDataloader(nb_sample=nb_sample, batch_size=batch_size), AxisMSELoss()\n)[0]\nprint(f\"Initial error: {l2_err:.2e}\")\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\nplt_model(net, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Growing MLP\nWe define a growing MLP model to approximate the sin function.\nDefine some auxiliary functions to train the model, and plot the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot():\n    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n    plt_model(model, ax)\n\n\nloss_func_sum = AxisMSELoss(reduction=\"sum\")\nloss_func_mean = AxisMSELoss(reduction=\"mean\")\n\n\ndef step(show=True, selected_layer=None, gamma_sample: int = 100):\n\n    train_dataloader = SinDataloader(nb_sample=nb_sample, batch_size=batch_size)\n\n    initial_loss, _ = compute_statistics(\n        growing_model=model, loss_function=loss_func_sum, dataloader=train_dataloader\n    )\n    print(f\"Initial loss: {initial_loss:.3e}\")\n\n    model.compute_optimal_update(part=\"all\", dtype=torch.float64)\n    model.reset_computation()\n    if selected_layer is None:\n        model.select_best_update(verbose=show)\n    else:\n        model.select_update(selected_layer, verbose=show)\n    # model.currently_updated_layer.delete_update()\n\n    selected_gamma, estimated_loss, x_sup, y_sup = line_search(\n        model=model,\n        loss_function=loss_func_sum,\n        dataloader=SinDataloader(nb_sample=nb_sample, batch_size=batch_size),\n        initial_loss=initial_loss,\n        first_order_improvement=model.updates_values[model.currently_updated_layer_index],\n        verbose=show,\n    )\n    x_sup = np.array(x_sup)\n    y_sup = np.array(y_sup)\n    print(f\"Selected gamma: {selected_gamma:.3e}, new loss: {estimated_loss:.3e}\")\n    print(\n        f\"Improvement: {initial_loss - estimated_loss:.3e}, fo improvement: {selected_gamma * model.currently_updated_layer.first_order_improvement.item():.3e}\"\n    )\n\n    if show:\n        window = min(5 * selected_gamma, 2 * max(x_sup))\n        x, y = full_search(\n            model=model,\n            loss=loss_func_sum,\n            dataloader=SinDataloader(nb_sample=nb_sample, batch_size=batch_size),\n            initial_loss=None,\n            first_order_improvement=model.updates_values[\n                model.currently_updated_layer_index\n            ],\n            min_value=-window,\n            max_value=window,\n            nb_points=gamma_sample,\n        )\n\n    model.amplitude_factor = np.sqrt(selected_gamma)\n    model.apply_update()\n\n    if show:\n        x_min = x[np.argmin(y)]\n        plt.axvline(x_min, color=\"red\", label=f\"Minimum {x_min:.3e}\")\n        plt.plot(x, y)\n        plt.plot(\n            x,\n            -x * model.currently_updated_layer.first_order_improvement.item()\n            + initial_loss,\n            label=f\"First order improvement {model.currently_updated_layer.first_order_improvement.item():.3e}\",\n        )\n        selected_sup = x_sup < window\n        plt.scatter(0, initial_loss, color=\"blue\", label=\"Initial loss\")\n        plt.scatter(\n            x_sup[selected_sup],\n            y_sup[selected_sup],\n            color=\"green\",\n            label=\"Line search\",\n            marker=\"x\",\n        )\n        plt.scatter(selected_gamma, estimated_loss, color=\"red\", label=\"Selected gamma\")\n        plt.ylim(0, 1.1 * max(y))\n        plt.legend()\n\n\ndef info():\n    loss, _ = evaluate_model(\n        model=model,\n        loss_function=loss_func_mean,\n        dataloader=SinDataloader(nb_sample=nb_sample, batch_size=batch_size),\n    )\n    print(f\"Loss: {loss:.3e}\")\n    plot()\n    for lay in model.layers:\n        print(lay)\n        # print(lay.weight)\n        print(f\"Min: {lay.weight.min().item()}, Max: {lay.weight.max().item()}\")\n    return model\n\n\nbatch_size = 1_000\nnb_sample = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = GrowingMLP(1, 1, 10, 2, activation=nn.SELU(), bias=True)\nmodel\n\ninfo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model for one step\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# step(gamma_sample=100)\n\n# info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another step\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# step()\n\n# info()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}